{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import cPickle as pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re, sys\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_dialogue_style(body):\n",
    "    ds_list = ['입니다.', '습니다.',  '합니다.', '됩니다.']\n",
    "    if body.find('입니다.') != -1:\n",
    "        if body.count('입니다.') == 1 and body.find('기사입니다.') != -1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    if body.find('습니다.') != -1:\n",
    "        if body.count('습니다.') == 1 and body.find('권리가 있습니다.') != -1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    if body.find('합니다.') != -1:\n",
    "        if body.count('합니다.') == 1 and body.find('재배포 가능합니다.') != -1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    if body.find('됩니다.') != -1:\n",
    "        if body.count('됩니다.') == 1 and body.find('게제됩니다.') != -1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### society case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../anaysis/201801-04_daum_society.csv'\n",
    "# filename = '../anaysis/201801-04_daum_entertain.csv'\n",
    "csv_reader = csv.reader(open(filename, 'r'))\n",
    "articles = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if n == 0: continue\n",
    "    articles.append([n-1, row[5], row[6]])\n",
    "        \n",
    "cln_articles = []\n",
    "n = 0\n",
    "for item in articles:\n",
    "    paras = [p for p in item[2].split('<p>') if p.strip()]\n",
    "    if len(paras) < 5: continue\n",
    "    body = '\\n\\n'.join(paras[2:-2])\n",
    "    if is_dialogue_style(body): continue\n",
    "    cln_articles.append([n, item[1], body])\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### entertain case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = '201801-04_daum_society.csv'\n",
    "filename = '../anaysis/201801-04_daum_entertain.csv'\n",
    "csv_reader = csv.reader(open(filename, 'r'))\n",
    "articles = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if n == 0: continue\n",
    "    articles.append([n-1, row[5], row[2]])\n",
    "    \n",
    "cln_articles = []\n",
    "print_articles = []\n",
    "n = 0\n",
    "for item in articles:\n",
    "    paras = [p for p in item[2].split('<p>') if p.strip()]\n",
    "    if len(paras) < 5: continue\n",
    "    body = '\\n\\n'.join(paras[2:-2])\n",
    "    ori_body = '\\n\\n'.join(paras)\n",
    "    if is_dialogue_style(body): continue\n",
    "    print_articles.append([n, item[1], ori_body])\n",
    "    cln_articles.append([n, item[1], body])\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_writer = csv.writer(open('tmp_' + filename, 'w'))\n",
    "for row in cln_articles:\n",
    "    csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(article_list):\n",
    "    cnt = 0   \n",
    "    llprint('  >> Tokenizing dataset...\\n')\n",
    "    \n",
    "    tkn_articles = []\n",
    "    for article in article_list:\n",
    "        cnt += 1\n",
    "        idx = article[0]\n",
    "        # head\n",
    "        head = article[1].decode('utf-8')\n",
    "        tag_head = twitter.morphs(head)\n",
    "        tkn_head = []\n",
    "        for tkn in tag_head:\n",
    "            tkn_head.append(tkn)\n",
    "        tkn_head = ' '.join(tkn_head)\n",
    "\n",
    "        # body\n",
    "        body = article[2]\n",
    "        tkn_body = []\n",
    "        for tap in body.split('\\n\\n'):\n",
    "            if not tap:\n",
    "                continue\n",
    "            else:\n",
    "                eos_list = re.finditer('[가-힣]+\\.\\s?|[가-힣]+\\s?\\)\\.\\s?', tap)\n",
    "                prev_eos = 0\n",
    "                for eos in eos_list:\n",
    "                    tkn_sent = []\n",
    "                    sentence = tap[prev_eos:eos.end()]\n",
    "                    prev_eos = eos.end()\n",
    "                    tag_sent = twitter.morphs(sentence.decode('utf-8'))\n",
    "                    for tkn in tag_sent:\n",
    "                        tkn_sent.append(tkn)\n",
    "                    tkn_sent.append(u'<EOS>')\n",
    "                    tkn_body.extend(tkn_sent)\n",
    "                if prev_eos < len(tap):\n",
    "                    tkn_sent = []\n",
    "                    sentence = tap[prev_eos:]\n",
    "                    tag_sent = twitter.morphs(sentence.decode('utf-8'))\n",
    "                    for tkn in tag_sent:\n",
    "                        tkn_sent.append(tkn)\n",
    "                    tkn_sent.append(u'<EOS>')\n",
    "                    tkn_body.extend(tkn_sent)\n",
    "            tkn_body.append(u'<EOP>')\n",
    "        tkn_body = ' '.join(tkn_body)\n",
    "        nn_tkn = [idx, tkn_head.encode('utf-8'), tkn_body.encode('utf-8')]\n",
    "        tkn_articles.append(nn_tkn)\n",
    "    \n",
    "    return tkn_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_length(data, max_len_t, max_len_b):\n",
    "    llprint('  >> Fitting length...\\n')\n",
    "    data_t, data_b = data\n",
    "    list_zeros = np.zeros(max_len_b, 'int32').tolist()\n",
    "    fl_data_t = []\n",
    "    for datum in data_t:\n",
    "        try:\n",
    "            datum = list(datum)\n",
    "        except:\n",
    "            pass\n",
    "        _len = len(datum)\n",
    "        if _len >= max_len_t:\n",
    "            fl_data_t.append( datum[:max_len_t] )\n",
    "        else:\n",
    "            fl_data_t.append( datum + list_zeros[:(max_len_t-_len)] )           \n",
    "    fl_data_b = []\n",
    "    for datum in data_b:\n",
    "        try:\n",
    "            datum = list(datum)\n",
    "        except:\n",
    "            pass\n",
    "        _len = len(datum)\n",
    "        if _len >= max_len_b:\n",
    "            fl_data_b.append( datum[:max_len_b] )\n",
    "        else:\n",
    "            fl_data_b.append( datum + list_zeros[:(max_len_b-_len)] )\n",
    "    np_data_t = np.asarray(fl_data_t, dtype='int32')\n",
    "    np_data_b = np.asarray(fl_data_b, dtype='int32')   \n",
    "    data = [np_data_t, np_data_b]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Tokenizing dataset...\n"
     ]
    }
   ],
   "source": [
    "tkn_articles = tokenize(cln_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_testset(tkn_articles, vocab_path):\n",
    "    llprint('  >> Converting words to idx...\\n')\n",
    "    tkn2idx = pickle.load(open(vocab_path, 'rb'))\n",
    "    str_ids = []\n",
    "    en_heads = []\n",
    "    en_paras = []\n",
    "    for _id, _head, _body in tkn_articles:\n",
    "        head_encodings= []\n",
    "        for tkn in _head.split():\n",
    "            if tkn in tkn2idx:\n",
    "                head_encodings.append(tkn2idx[tkn])\n",
    "            else:\n",
    "                head_encodings.append(1) # DO NOT CHANGE: index of <UNK> \n",
    "        for para in _body.split('<EOP>'):\n",
    "            if para != ' ' and para:\n",
    "                para = para + '<EOP>'\n",
    "                para_encodings = []\n",
    "                for tkn in para.split():\n",
    "                    if tkn != ' ' and tkn:\n",
    "                        if tkn in tkn2idx:\n",
    "                            para_encodings.append(tkn2idx[tkn])\n",
    "                        else:\n",
    "                            para_encodings.append(1) # DO NOT CHANGE: index of <UNK> \n",
    "                str_ids.append(str(_id))\n",
    "                en_heads.append(head_encodings)\n",
    "                en_paras.append(para_encodings)\n",
    "    if len(str_ids) != len(en_heads) and len(str_ids) != len(en_paras):\n",
    "        raise NotImplementedError('Please Check Some Codes')\n",
    "        \n",
    "    return str_ids, en_heads, en_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path =  '../dic_mincutN.pkl'\n",
    "output_path = 'test_para_' + filename + '.pkl'\n",
    "max_len_head = 49\n",
    "max_len_para = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Converting words to idx...\n",
      "  >> Fitting length...\n",
      "  >> Saving preprocessed dataset in (test_para_201801-04_daum_society.csv.pkl)...\n"
     ]
    }
   ],
   "source": [
    "str_ids, en_heads, en_paras = encoding_testset(tkn_articles, vocab_path)\n",
    "[np_heads, np_paras] = fit_length([en_heads, en_paras], max_len_head, max_len_para)\n",
    "llprint('  >> Saving preprocessed dataset in (%s)...\\n' % output_path)\n",
    "pickle.dump([str_ids, np_heads, np_paras], open(output_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_whole_testset(tkn_articles, vocab_path):\n",
    "    llprint('  >> Converting words to idx...\\n')\n",
    "    tkn2idx = pickle.load(open(vocab_path, 'rb'))\n",
    "    str_ids = []\n",
    "    en_heads = []\n",
    "    en_bodys = []\n",
    "    for _id, _head, _body in tkn_articles:\n",
    "        head_encodings = []\n",
    "        for tkn in _head.split():\n",
    "            if tkn in tkn2idx:\n",
    "                head_encodings.append(tkn2idx[tkn])\n",
    "            else:\n",
    "                head_encodings.append(1) # DO NOT CHANGE: index of <UNK> \n",
    "        body_encodings = []\n",
    "        for tkn in _body.split():\n",
    "            if tkn != ' ' and tkn:\n",
    "                if tkn in tkn2idx:\n",
    "                    body_encodings.append(tkn2idx[tkn])\n",
    "                else:\n",
    "                    body_encodings.append(1) # DO NOT CHANGE: index of <UNK> \n",
    "        str_ids.append(_id)\n",
    "        en_heads.append(head_encodings)\n",
    "        en_bodys.append(body_encodings)\n",
    "    if len(str_ids) != len(en_heads) and len(str_ids) != len(en_bodys):\n",
    "        raise NotImplementedError('Please Check Some Codes')\n",
    "        \n",
    "    return str_ids, en_heads, en_bodys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path =  '../dic_mincutN.pkl'\n",
    "output_path = 'test_whole_' + filename + '.pkl'\n",
    "max_len_head = 49\n",
    "max_len_body = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Converting words to idx...\n",
      "  >> Fitting length...\n",
      "  >> Saving preprocessed dataset in (test_whole_201801-04_daum_society.csv.pkl)...\n"
     ]
    }
   ],
   "source": [
    "str_ids, en_heads, en_bodys = encoding_whole_testset(tkn_articles, vocab_path)\n",
    "[np_heads, np_bodys] = fit_length([en_heads, en_bodys], max_len_head, max_len_body)\n",
    "llprint('  >> Saving preprocessed dataset in (%s)...\\n' % output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([str_ids, np_heads, np_bodys], open(output_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578269 136129\n",
      "유행렬 청와대 행정관 \"청주시장 출마 긍정적으로 고민 중\"\n",
      "그는 \"이제는 청와대에 남아 지역발전을 위해 노력할 것인 지 아니면 청주시장 출마가 지역에 도움이 되는 지 등으로 가닥을 잡고 생각하고 있다\"고 말했다.\n",
      "\n",
      "이어 \"이번 설 명절 때 가족들과 상의해 결정하려 한다\"며 \"가족들이 적극적으로 협조를 해야 저도 편하게 결정하고 (출마를)실행할 수 있다\"고 강조했다.\n",
      "\n",
      "최근 한 언론사의 여론조사에서 1위에 오른 것이 '외부 효과'란 지적과 관련해 \"지금의 현재 모습으로 그 정도 평가가 나온 것\"이라며 선을 분명히 그었다.\n",
      "\n",
      "유 행정관은 \"저는 20여 년 동안 지역에서 오랜 인맥을 만들고 바닥에서부터 실무를 다져왔다\"며 \"청와대에서 근무하게 된 것도 실력이 있다는 검증을 받은 것\"이라고 강조했다.\n",
      "\n",
      "또 \"사람을 평가할 때 현재가 과거와 분리돼 평가받는 것이 아니라 총체적인 것으로 생각한다\"며 \"인지도와 지지도는 개념 자체가 다르며 지금 모습으로 충분히 평가가 나온 것\"이라고 주장했다.\n",
      "\n",
      "'문 대통령의 최측근으로 분류되는 노영민 주중대사와 교감이 있었냐'는 질문에는 \"(서로)어려운 문제가 있으면 상의하지만 허락을 하고 받는 관계는 아니다\"라고 말했다.\n"
     ]
    }
   ],
   "source": [
    "print len(cln_articles),\n",
    "i = random.randint(0, len(cln_articles))\n",
    "print i\n",
    "print cln_articles[i][1]\n",
    "print cln_articles[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(open('tmp_' + filename, 'r'))\n",
    "articles = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if n == 0: continue\n",
    "    articles.append([row[0], row[1], row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open test.pkl file\n",
    "[str_ids, np_heads, np_paras] = pickle.load(open('../test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5699"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(open(filename, 'r'))\n",
    "articles = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if n == 0: continue\n",
    "    articles.append([n-1, row[5], row[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1309163\n",
      "[제천소식]이시종 충북지사 제천서 장보기 등\n",
      "<p>【제천=뉴시스】강신욱 기자 = ◇이시종 충북지사 제천서 장보기<p> <p>이시종 충북도지사는 14일 화재 참사로 어려움을 겪는 제천을 방문했다.<p> <p>이 지사는 이날 내토시장을 찾아 설 명절을 앞두고 장보기 행사를 했다.<p> <p>이 지사는 제천소방서를 방문해 직원들을 격려하고 사회복지시설을 찾아 어르신들을 위로했다.<p> <p>이어 충주로 이동해 1961년 준공된 구 목행대교에 대한 국가안전대진단 특별점검을 했다.<p> <p>◇제천소방서, 안전 홍보 캠페인<p> <p>충북 제천소방서(서장 김상현)는 14일 오후 제천역전시장과 시외버스터미널에서 '고향집, 주택용 소방시설 선물하기 홍보 캠페인'을 했다.<p> <p>이날 캠페인은 주택용 소방시설 설치 촉진과 자율 설치 환경 조성을 위해 귀성객을 대상으로 진행했다.<p> <p>제천소방서는 소방시설(소화기·감지기) 선물과 소방차 길 터주기 캠페인도 함께했다.◇세명대 간호학과 국가고시 100％ 합격<p> <p>충북 제천 세명대학교는 간호학과가 간호사 국가고시에 100％에 합격했다고 14일 밝혔다.<p> <p>세명대 간호학과는 최근 한국보건의료인국가시험원에서 시행한 58회 간호사 국가고시에 졸업예정자 108명이 응시했다.<p> <p>이번 졸업자 96.3％가 강남세브란스병원, 서울대병원, 삼성서울병원, 서울아산병원, 서울성모병원 등에 취업했다.<p> <p>ksw64@newsis.com<p>\n"
     ]
    }
   ],
   "source": [
    "print len(articles)\n",
    "print articles[0][1]\n",
    "print articles[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341058\n",
      "김소희, '이윤택 성폭력' 피해자의 \"등 떠밀었다\" 폭로에 \"사실 아니다\"\n",
      "<p>지난 19일 '뉴스룸'에서는 이윤택씨에게 성폭력을 당했다고 주장한 피해자가 실명을 밝히지 않고 목소리를 변조해 손석희 앵커와 인터뷰를 가졌다. 피해자는 \"2004, 2005년 정도부터 성폭력 피해를 입었다\"며 \"안마라는 이름으로 수위를 넘어서는 행위를 강요받았다\"고 주장했다.<p> <p>이어 피해자는 “(이윤택이) 나는 너와 자고 싶다”며 가슴으로 손이 쑥 들어와 급하게 피한 적도 있고 “발성을 키워야 된다면서 사타구니 쪽에 나무젓가락을 꽂은 적도 있었다”고 폭로했다.<p> <p>피해자는 \"안마를 거부하면 전체 단원을 모은 뒤 거부한 한명을 두고 마녀사냥하듯 거부한 여자 단원에 대한 안 좋은 점을 이야기했다. 그 전에 캐스팅됐던 역할에서 배제시켰다\"고 밝혔다.<p> <p>이어 이 피해자는 \"나에게 ‘이윤택이 안마를 원한다 들어가라’며 등을 떠민 건 여자선배였다\"고 증언했다. 그는 \"김소희 대표가 조력자처럼 후배를 초이스하고 안마를 권유했다\"며 \"나에게 과일이 든 쟁반을 주면서 이윤택 방에 가서 안마를 하라고 했다. 내가 거부하자 가슴팍을 치면서 왜 이렇게 이기적이냐 너만 희생하면 되는데 왜 그러냐고 말했다. 아직까지 그 눈빛이 잊혀지지 않는다\"고 주장했다.<p> <p>이에 대해 김 대표는 19일 지인의 SNS를 통해 \"저희 극단이 잘못한 일로 책임감은 크지만 JTBC 뉴스에 나온 내용은 전혀 사실이 아니다\"고 밝혔다. 연희단거리패는 이날 해체 발표 이후 공식 홈페이지와 SNS를 모두 폐쇄해 지인의 SNS를 통해 입장을 밝힌 것으로 보인다.<p> <p>김소희 대표는 이 같은 증언에 대해 \"저도 너무 놀라 손이 떨린다. 방송국 측에 정정신청 해놓았다\"고 밝혔다. 이어 \"인터뷰한 사람이 누군지 모르겠지만 사실을 밝히는 데 필요한 조치가 있다면 다 할 것이다\"고 덧붙였다.<p>\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(articles))\n",
    "print i\n",
    "print articles[i][1]\n",
    "print articles[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_articles = []\n",
    "n = 0\n",
    "for item in articles:\n",
    "    paras = [p for p in item[2].split('<p>') if p.strip()]\n",
    "    if len(paras) < 3: continue\n",
    "    body = '\\n\\n'.join(paras[1:-1])\n",
    "    cleaned_articles.append([n, item[1], body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608249\n",
      "교통사고 사망자 5년 연속 감소..보행자 사망 사고 비율은 여전히 높아\n",
      "6일 경찰청이 발표한 2017년 교통사고 분석에 따르면 작년 한 해 교통사고 사망자는 4185명으로 전년(4292명) 대비 2.5%(107명) 줄어들었다. 교통사고 사망자는 지난 2012년 5392명에서 지난해까지 5년 연속 감소했다.\n",
      "\n",
      "교통사고 사망자는 감소세를 나타내고 있지만 보행자 사망 비율은 여전히 높은 것으로 나타났다. 지난해 교통사고로 숨진 보행자는 1675명으로 전체 사망사고에서 40%를 차지했다. 2016년 기준 경제협력개발기구(OECD) 보행 사망자 비율(19.2%)의 두 배를 웃도는 수치다. 특히 노인 보행자 사망은 906명으로 전년보다 40명(4.6%) 증가해 전체 보행자 사망의 54.1%에 달했다.\n",
      "\n",
      "경찰 관계자는 \"OECD 등 선진국에서 교통사고 사망자수 감소에 성과를 거둔 '안전속도 5030' 시범운영을 확대하는 등 보행자 친화적인 정책을 시행할 것\"이라며 \"2012년 이후 교통사고 사망자가 지속해서 감소하고 있고 올해는 3000명대로 떨어질 것으로 기대된다\"고 전했다. 안전속도5030은 도시 내 제한속도를 50km/h로 하고, 보호구역 등 특별보호 필요 지역은 30km/h로 지정하는 속도관리정책이다.\n",
      "\n",
      "최근 3년간 증가했던 어린이 사망자는 지난해 54명으로 전년(71명)보다 17명(23.9%) 줄었다. 스쿨존(어린이보호구역) 사망자는 전년과 같은 8명이었으나 부상자는 23명 감소한 487명이었고, 통학버스 사망자는 없었다.\n",
      "\n",
      "음주운전 사망자는 439명으로 전년(481명)보다 8.7%(42명) 감소하는 등 2012년 이후 지속적으로 감소세를 보인 것으로 나타났다.\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(cleaned_articles))\n",
    "print i\n",
    "print cleaned_articles[i][1]\n",
    "print cleaned_articles[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(open('20180501-02_naver.csv', 'r'))\n",
    "articles = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if n == 0: continue\n",
    "    articles.append([n-1, row[4], row[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(articles)\n",
    "print articles[0][1]\n",
    "print articles[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(articles))\n",
    "print i\n",
    "print articles[i][1]\n",
    "print articles[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(open('20180501-02_daum.csv', 'r'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([str_ids, np_heads, np_paras], open(output_path,'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf114_p27]",
   "language": "python",
   "name": "conda-env-tf114_p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
